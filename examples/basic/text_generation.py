#!/usr/bin/env python3
"""
æ–‡æœ¬ç”ŸæˆåŸºç¡€ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Gemini MCP æœåŠ¡è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚
åŒ…æ‹¬ä¸åŒç±»å‹çš„æ–‡æœ¬ç”Ÿæˆï¼šåˆ›ä½œã€ç¿»è¯‘ã€æ‘˜è¦ç­‰ã€‚

è¿è¡Œç¤ºä¾‹:
    python examples/basic/text_generation.py

ç¯å¢ƒå˜é‡:
    GEMINI_API_KEY: Gemini APIå¯†é’¥ (å¿…éœ€)
    OUTPUT_DIR: è¾“å‡ºç›®å½• (å¯é€‰ï¼Œé»˜è®¤: ./output)
    LOG_LEVEL: æ—¥å¿—çº§åˆ« (å¯é€‰ï¼Œé»˜è®¤: info)
"""

import asyncio
import os
import sys
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.gemini_kling_mcp.tools.text_generation import generate_text
from src.gemini_kling_mcp.services.gemini.models import GeminiModel

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO').upper()),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def setup_output_directory() -> Path:
    """è®¾ç½®è¾“å‡ºç›®å½•"""
    output_dir = Path(os.getenv('OUTPUT_DIR', './output'))
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


async def creative_writing_example():
    """åˆ›æ„å†™ä½œç¤ºä¾‹"""
    logger.info("å¼€å§‹åˆ›æ„å†™ä½œç¤ºä¾‹...")
    
    prompts = [
        {
            "name": "ç§‘å¹»å°æ•…äº‹",
            "prompt": "å†™ä¸€ä¸ª200å­—å·¦å³çš„ç§‘å¹»å°æ•…äº‹ï¼Œå…³äºä¸€ä¸ªæœºå™¨äººå‘ç°äº†æƒ…æ„Ÿçš„æ•…äº‹ã€‚",
            "max_tokens": 300,
            "temperature": 0.8
        },
        {
            "name": "è¯—æ­Œåˆ›ä½œ",
            "prompt": "åˆ›ä½œä¸€é¦–å…³äºæ˜¥å¤©çš„ç°ä»£è¯—ï¼Œè¡¨è¾¾å¯¹è‡ªç„¶çš„èµç¾ã€‚",
            "max_tokens": 200,
            "temperature": 0.9
        },
        {
            "name": "äº§å“æè¿°",
            "prompt": "ä¸ºä¸€æ¬¾æ™ºèƒ½æ‰‹è¡¨å†™ä¸€æ®µæœ‰å¸å¼•åŠ›çš„äº§å“æè¿°ï¼Œçªå‡ºå…¶å¥åº·ç›‘æµ‹åŠŸèƒ½ã€‚",
            "max_tokens": 150,
            "temperature": 0.7
        }
    ]
    
    results = []
    for prompt_info in prompts:
        try:
            logger.info(f"ç”Ÿæˆ: {prompt_info['name']}")
            
            result = await generate_text(
                prompt=prompt_info["prompt"],
                model=GeminiModel.GEMINI_15_FLASH,
                max_tokens=prompt_info["max_tokens"],
                temperature=prompt_info["temperature"]
            )
            
            if result["success"]:
                results.append({
                    "name": prompt_info["name"],
                    "prompt": prompt_info["prompt"],
                    "response": result["text"],
                    "model": result["model"]
                })
                logger.info(f"âœ… {prompt_info['name']} ç”ŸæˆæˆåŠŸ")
            else:
                logger.error(f"âŒ {prompt_info['name']} ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            logger.error(f"âŒ {prompt_info['name']} ç”Ÿæˆå¼‚å¸¸: {e}")
    
    return results


async def text_processing_example():
    """æ–‡æœ¬å¤„ç†ç¤ºä¾‹"""
    logger.info("å¼€å§‹æ–‡æœ¬å¤„ç†ç¤ºä¾‹...")
    
    source_text = """
    äººå·¥æ™ºèƒ½(Artificial Intelligenceï¼ŒAI)æ˜¯ä¸€é—¨æå¯ŒæŒ‘æˆ˜æ€§çš„ç§‘å­¦ï¼Œä»äº‹è¿™é¡¹å·¥ä½œçš„äººå¿…é¡»æ‡‚å¾—è®¡ç®—æœºçŸ¥è¯†ã€
    å¿ƒç†å­¦å’Œå“²å­¦ã€‚äººå·¥æ™ºèƒ½æ˜¯åŒ…æ‹¬ååˆ†å¹¿æ³›çš„ç§‘å­¦ï¼Œå®ƒç”±ä¸åŒçš„é¢†åŸŸç»„æˆï¼Œå¦‚æœºå™¨å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ç­‰ç­‰ï¼Œ
    æ€»çš„è¯´æ¥ï¼Œäººå·¥æ™ºèƒ½ç ”ç©¶çš„ä¸€ä¸ªä¸»è¦ç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿèƒœä»»ä¸€äº›é€šå¸¸éœ€è¦äººç±»æ™ºèƒ½æ‰èƒ½å®Œæˆçš„å¤æ‚å·¥ä½œã€‚
    """
    
    tasks = [
        {
            "name": "æ–‡æœ¬æ‘˜è¦",
            "prompt": f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼š\n\n{source_text}",
            "max_tokens": 100
        },
        {
            "name": "å…³é”®è¯æå–",
            "prompt": f"ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼š\n\n{source_text}",
            "max_tokens": 50
        },
        {
            "name": "æ–‡æœ¬ç¿»è¯‘",
            "prompt": f"å°†ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š\n\n{source_text.strip()}",
            "max_tokens": 200
        }
    ]
    
    results = []
    for task in tasks:
        try:
            logger.info(f"å¤„ç†: {task['name']}")
            
            result = await generate_text(
                prompt=task["prompt"],
                model=GeminiModel.GEMINI_15_FLASH,
                max_tokens=task["max_tokens"],
                temperature=0.3  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            )
            
            if result["success"]:
                results.append({
                    "name": task["name"],
                    "response": result["text"]
                })
                logger.info(f"âœ… {task['name']} å¤„ç†æˆåŠŸ")
            else:
                logger.error(f"âŒ {task['name']} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            logger.error(f"âŒ {task['name']} å¤„ç†å¼‚å¸¸: {e}")
    
    return results


async def conversational_example():
    """å¯¹è¯å¼ç”Ÿæˆç¤ºä¾‹"""
    logger.info("å¼€å§‹å¯¹è¯å¼ç”Ÿæˆç¤ºä¾‹...")
    
    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    conversation_history = []
    
    conversation_turns = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„åŸºç¡€çŸ¥è¯†ã€‚",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "èƒ½ç»™æˆ‘æ¨èä¸€äº›å­¦ä¹ èµ„æºå—ï¼Ÿ",
        "è°¢è°¢ä½ çš„å»ºè®®ï¼"
    ]
    
    results = []
    for i, user_input in enumerate(conversation_turns):
        try:
            logger.info(f"å¯¹è¯è½®æ¬¡ {i+1}: {user_input[:30]}...")
            
            # æ„å»ºåŒ…å«å†å²çš„æç¤º
            context = "\n".join([
                f"ç”¨æˆ·: {turn['user']}\nåŠ©æ‰‹: {turn['assistant']}" 
                for turn in conversation_history
            ])
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½ä¸”çŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å¯¹è¯å†å²:
{context}

ç”¨æˆ·: {user_input}
åŠ©æ‰‹:"""
            
            result = await generate_text(
                prompt=prompt,
                model=GeminiModel.GEMINI_15_FLASH,
                max_tokens=200,
                temperature=0.7
            )
            
            if result["success"]:
                assistant_response = result["text"].strip()
                
                # æ›´æ–°å¯¹è¯å†å²
                conversation_history.append({
                    "user": user_input,
                    "assistant": assistant_response
                })
                
                results.append({
                    "turn": i + 1,
                    "user": user_input,
                    "assistant": assistant_response
                })
                
                logger.info(f"âœ… å¯¹è¯è½®æ¬¡ {i+1} å®Œæˆ")
            else:
                logger.error(f"âŒ å¯¹è¯è½®æ¬¡ {i+1} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                break
                
        except Exception as e:
            logger.error(f"âŒ å¯¹è¯è½®æ¬¡ {i+1} å¼‚å¸¸: {e}")
            break
    
    return results


async def specialized_generation_example():
    """ä¸“ä¸šé¢†åŸŸç”Ÿæˆç¤ºä¾‹"""
    logger.info("å¼€å§‹ä¸“ä¸šé¢†åŸŸç”Ÿæˆç¤ºä¾‹...")
    
    specialized_tasks = [
        {
            "name": "ä»£ç ç”Ÿæˆ",
            "prompt": "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œå®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼ŒåŒ…å«è¯¦ç»†æ³¨é‡Šã€‚",
            "max_tokens": 300,
            "temperature": 0.2
        },
        {
            "name": "æŠ€æœ¯æ–‡æ¡£",
            "prompt": "ä¸ºREST APIç¼–å†™ä¸€ä¸ªç®€å•çš„ä½¿ç”¨æ–‡æ¡£ï¼ŒåŒ…æ‹¬ç«¯ç‚¹ã€å‚æ•°å’Œç¤ºä¾‹ã€‚",
            "max_tokens": 400,
            "temperature": 0.3
        },
        {
            "name": "å•†ä¸šææ¡ˆ",
            "prompt": "ä¸ºä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°å†™ä¸€æ®µå•†ä¸šææ¡ˆçš„æ‰§è¡Œæ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰ã€‚",
            "max_tokens": 250,
            "temperature": 0.6
        },
        {
            "name": "å­¦æœ¯å†™ä½œ",
            "prompt": "å†™ä¸€æ®µå…³äºæœºå™¨å­¦ä¹ åœ¨åŒ»ç–—è¯Šæ–­ä¸­åº”ç”¨çš„å­¦æœ¯è®ºæ–‡æ‘˜è¦ã€‚",
            "max_tokens": 200,
            "temperature": 0.4
        }
    ]
    
    results = []
    for task in specialized_tasks:
        try:
            logger.info(f"ç”Ÿæˆ: {task['name']}")
            
            result = await generate_text(
                prompt=task["prompt"],
                model=GeminiModel.GEMINI_15_FLASH,
                max_tokens=task["max_tokens"],
                temperature=task["temperature"]
            )
            
            if result["success"]:
                results.append({
                    "name": task["name"],
                    "response": result["text"],
                    "tokens": result.get("token_count", 0),
                    "model": result["model"]
                })
                logger.info(f"âœ… {task['name']} ç”ŸæˆæˆåŠŸ")
            else:
                logger.error(f"âŒ {task['name']} ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            logger.error(f"âŒ {task['name']} ç”Ÿæˆå¼‚å¸¸: {e}")
    
    return results


def save_results_to_file(results: dict, output_dir: Path):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"text_generation_results_{timestamp}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ç»“æœ\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 50 + "\n\n")
        
        for category, items in results.items():
            f.write(f"## {category}\n\n")
            
            if category == "å¯¹è¯å¼ç”Ÿæˆ":
                for item in items:
                    f.write(f"è½®æ¬¡ {item['turn']}:\n")
                    f.write(f"ç”¨æˆ·: {item['user']}\n")
                    f.write(f"åŠ©æ‰‹: {item['assistant']}\n\n")
            else:
                for item in items:
                    f.write(f"### {item['name']}\n")
                    if 'prompt' in item:
                        f.write(f"æç¤º: {item['prompt'][:100]}...\n")
                    f.write(f"å“åº”: {item['response']}\n")
                    if 'model' in item:
                        f.write(f"æ¨¡å‹: {item['model']}\n")
                    f.write("\n" + "-" * 30 + "\n\n")
    
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv('GEMINI_API_KEY'):
        logger.error("âŒ è¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_output_directory()
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    try:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        results = {}
        
        # åˆ›æ„å†™ä½œç¤ºä¾‹
        creative_results = await creative_writing_example()
        if creative_results:
            results["åˆ›æ„å†™ä½œ"] = creative_results
        
        # æ–‡æœ¬å¤„ç†ç¤ºä¾‹
        processing_results = await text_processing_example()
        if processing_results:
            results["æ–‡æœ¬å¤„ç†"] = processing_results
        
        # å¯¹è¯å¼ç”Ÿæˆç¤ºä¾‹
        conversation_results = await conversational_example()
        if conversation_results:
            results["å¯¹è¯å¼ç”Ÿæˆ"] = conversation_results
        
        # ä¸“ä¸šé¢†åŸŸç”Ÿæˆç¤ºä¾‹
        specialized_results = await specialized_generation_example()
        if specialized_results:
            results["ä¸“ä¸šé¢†åŸŸç”Ÿæˆ"] = specialized_results
        
        # ä¿å­˜ç»“æœ
        if results:
            save_results_to_file(results, output_dir)
            
            # æ‰“å°æ‘˜è¦
            total_successful = sum(len(items) for items in results.values())
            logger.info(f"âœ… ç¤ºä¾‹è¿è¡Œå®Œæˆï¼æˆåŠŸç”Ÿæˆ {total_successful} ä¸ªæ–‡æœ¬")
            logger.info("ğŸ“ æŸ¥çœ‹è¯¦ç»†ç»“æœï¼Œè¯·æ‰“å¼€è¾“å‡ºæ–‡ä»¶")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸçš„ç”Ÿæˆç»“æœ")
    
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå‡ºé”™: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())